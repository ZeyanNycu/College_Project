{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7107371,"sourceType":"datasetVersion","datasetId":4097623}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader,TensorDataset\nimport pandas as pd\nimport torch\nimport re\nimport numpy as np\nimport os\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nclass dataset(Dataset):\n    def __init__(self, Data_path):\n        # We should use source id to classify the conservation\n        df = pd.read_csv(Data_path, sep='\\t')\n        self.data = []\n        self.mapping = {'MS':0,\n                       'PH':1,\n                       'AM':2,\n                       'SF':3,\n                       'SR':4,\n                       'OTHER':5}\n        self.GT = []\n        text = []\n        repeat = { 'AM': 7,\n                   'SR': 6,\n                   'SF': 4,\n                   'MS': 4,\n                   'PH': 4,\n                 'OTHER':1}\n        times = 0\n        for _, row in df.iterrows():\n            if('AM' in row['classes']):\n                times = repeat['AM']\n            elif('SR' in row['classes']):\n                times = repeat['SR']\n            elif('SF' in row['classes']):\n                times = repeat['SF']\n            elif('MS' in row['classes']):\n                times = repeat['MS']\n            elif('PH' in row['classes']):\n                times = repeat['PH']\n            else:\n                times = repeat['OTHER']\n            for i in range(times):\n                text.append(row['utterance'])\n                train_data = self.get_train_data(text)\n                self.data.append(train_data)\n                GT = np.zeros((6))\n                if('MS' in row['classes']):\n                    GT[0] = 1\n                if('PH' in row['classes']):\n                    GT[1] = 1\n                if('AM' in row['classes']):\n                    GT[2] = 1\n                if('SF' in row['classes']):\n                    GT[3] = 1\n                if('SR' in row['classes']):\n                    GT[4] = 1\n                if('OTHER' in row['classes']):\n                    GT[5] = 1\n                self.GT.append(GT)\n        \n    def __len__(self):\n        return len(self.data)\n    def get_train_data(self,text):\n        ans = \"\"\n        if(len(text)<3):\n            for i in text:\n                ans += i\n        else:\n            for i in range(3):\n                ans += text[-3+i]\n        return ans \n    def __getitem__(self, idx):\n        data,GT = self.data[idx],self.GT[idx]\n        return data,GT","metadata":{"execution":{"iopub.status.busy":"2023-12-04T10:07:09.588398Z","iopub.execute_input":"2023-12-04T10:07:09.588981Z","iopub.status.idle":"2023-12-04T10:07:13.427285Z","shell.execute_reply.started":"2023-12-04T10:07:09.588954Z","shell.execute_reply":"2023-12-04T10:07:13.426318Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import requests\nimport argparse\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification\nfrom torch.utils.tensorboard import SummaryWriter\nfrom transformers import DebertaTokenizer, DebertaForSequenceClassification,RobertaTokenizer, RobertaForSequenceClassification, XLNetTokenizer, XLNetForSequenceClassification\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer,AutoModelForSequenceClassification, AutoTokenizer,T5Tokenizer, T5ForConditionalGeneration\n\n\nwriter = SummaryWriter(\"/kaggle/working/logs\")\nnum_classes = 6\nnum_epochs=5\nbatch_size=16\nlearning_rate=0.001\nName='baseline'\ndevice='cuda'\nmodel_path='t5-small' \ndef train(train_path,eval_path):\n    ##Step 1 construct training dataset\n    eval_dataset = dataset(eval_path)\n    train_dataset = dataset(train_path)\n#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n#     tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n#     tokenizer = RobertaTokenizer.from_pretrained(model_path)\n#     tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n#     tokenizer = AutoTokenizer.from_pretrained(model_path)\n    tokenizer = T5Tokenizer.from_pretrained(model_path)\n    train_tokens = tokenizer.batch_encode_plus(\n                    train_dataset.data,\n                    padding=True,\n                    truncation=True,\n                    return_tensors='pt',\n    )\n    test_tokens = tokenizer.batch_encode_plus(\n        eval_dataset.data,\n        padding=True,\n        truncation=True,\n        return_tensors='pt',\n    )\n    train_labels = torch.tensor(train_dataset.GT)\n    test_labels = torch.tensor(eval_dataset.GT)\n    # Create DataLoader\n    train_data = TensorDataset(train_tokens['input_ids'], train_tokens['attention_mask'], train_labels)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_data = TensorDataset(test_tokens['input_ids'], test_tokens['attention_mask'], test_labels)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n#     model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes).to(device)\n#     model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=num_classes).to(device)\n#     model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_classes).to(device)\n#     model = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels=num_classes).to(device)\n#     model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_classes).to(device)\n    model = T5ForConditionalGeneration.from_pretrained(model_path, num_labels=num_classes).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), \n                                    lr=5e-05, # this learning rate is for bert model , )taken from huggingface website \n                                    eps=1e-08,\n                                    weight_decay=0.01)\n    loss_fn = torch.nn.CrossEntropyLoss()\n    loss_min = np.inf\n    # Fine-tune the model\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        loss_all = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids, attention_mask, labels = batch\n            outputs = model(input_ids.to(device), attention_mask=attention_mask.to(device), labels=labels.to(device))\n            loss = outputs.loss\n            loss_all += loss\n            loss.backward()\n            optimizer.step()\n        writer.add_scalar(f\"Training_Loss/{Name}\", loss_all/len(train_loader), epoch)\n        model.eval()\n        loss_all = 0\n        with torch.no_grad():\n            for batch in test_loader:\n                input_ids, attention_mask, labels = batch\n                outputs = model(input_ids.to(device), attention_mask=attention_mask.to(device), labels=labels.to(device))\n                loss = outputs.loss\n                loss_all += loss\n            writer.add_scalar(f\"Validate_Loss/{Name}\", loss_all/len(test_loader), epoch)\n            print(loss_all/len(train_loader))\n            if(loss_all/len(train_loader)<loss_min):\n                loss_min = loss_all/len(train_loader)\n                model.save_pretrained('/kaggle/working/model.pt')\n                tokenizer.save_pretrained('/kaggle/working/tokenizer.pt')\ndef Show_proposition(train_path):\n    data = pd.read_csv(train_path, sep='\\t')\n    GT = np.zeros(6)\n    s = len(data)\n    print(s)\n    for index, row in data.iterrows():\n        if('MS' in row['classes']):\n            GT[0] += 1\n        if('PH' in row['classes']):\n            GT[1] += 1\n        if('AM' in row['classes']):\n            GT[2] += 1\n        if('SF' in row['classes']):\n            GT[3] += 1\n        if('SR' in row['classes']):\n            GT[4] += 1\n        if('OTHER' in row['classes']):\n            GT[5] += 1\n    print(f'MS:{GT[0]/s}, PH:{GT[1]/s}, AM:{GT[2]/s}, SF:{GT[3]/s},SR:{GT[4]/s}, OTHER:{GT[5]/s}')\n\nif __name__ == '__main__':\n    mode='train'\n    train_path='/kaggle/input/nlp-hw2/train.tsv'\n    eval_path='/kaggle/input/nlp-hw2/val.tsv'\n    if(mode == 'train'):\n        print(\"Start to use train mode\")\n        train(train_path,eval_path)\n    elif(mode == \"analysis\"):\n        Show_proposition(train_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-04T10:41:35.841700Z","iopub.execute_input":"2023-12-04T10:41:35.842361Z","iopub.status.idle":"2023-12-04T10:48:30.999578Z","shell.execute_reply.started":"2023-12-04T10:41:35.842329Z","shell.execute_reply":"2023-12-04T10:48:30.998658Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Start to use train mode\n","output_type":"stream"},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"756938c47a214cadac484b8c4b965466"}},"metadata":{}},{"name":"stderr","text":"Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at nghuyong/ernie-2.0-en and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"tensor(0.0804, device='cuda:0', dtype=torch.float64)\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 2/5 [02:43<04:05, 81.79s/it]","output_type":"stream"},{"name":"stdout","text":"tensor(0.0903, device='cuda:0', dtype=torch.float64)\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 3/5 [04:05<02:43, 81.63s/it]","output_type":"stream"},{"name":"stdout","text":"tensor(0.1102, device='cuda:0', dtype=torch.float64)\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 4/5 [05:26<01:21, 81.55s/it]","output_type":"stream"},{"name":"stdout","text":"tensor(0.1103, device='cuda:0', dtype=torch.float64)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5/5 [06:48<00:00, 81.60s/it]","output_type":"stream"},{"name":"stdout","text":"tensor(0.1314, device='cuda:0', dtype=torch.float64)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\ndef metric(predict,GT):\n    TP,FP,FN = np.zeros((6)),np.zeros((6)),np.zeros((6))\n    for i in range(len(GT[0])):\n        if(GT[0][i] == 0 and predict[i] == 1):\n            FP[i] += 1\n        elif(GT[0][i] == 1):\n            if(predict[i] == 1):\n                TP[i] += 1\n            else:\n                FN[i] += 1\n    return TP,FP,FN\neval_path='/kaggle/input/nlp-hw2/val.tsv'\neval_dataset = dataset(eval_path)\n# model = BertForSequenceClassification.from_pretrained('/kaggle/working/model.pt').to(device)\n# tokenizer = BertTokenizer.from_pretrained('/kaggle/working/tokenizer.pt')\n\n# model = DebertaForSequenceClassification.from_pretrained('/kaggle/working/model.pt').to(device)\n# tokenizer = DebertaTokenizer.from_pretrained('/kaggle/working/tokenizer.pt')\n# model = XLNetForSequenceClassification.from_pretrained('/kaggle/working/model.pt').to(device)\n# tokenizer =  XLNetTokenizer.from_pretrained('/kaggle/working/tokenizer.pt')\n# model = RobertaForSequenceClassification.from_pretrained('/kaggle/working/model.pt').to(device)\n# tokenizer =  RobertaTokenizer.from_pretrained('/kaggle/working/tokenizer.pt')\n# model = DistilBertForSequenceClassification.from_pretrained('/kaggle/working/model.pt').to(device)\n# tokenizer =  DistilBertTokenizer.from_pretrained('/kaggle/working/tokenizer.pt')\nmodel = AutoModelForSequenceClassification.from_pretrained('/kaggle/working/model.pt').to(device)\ntokenizer = AutoTokenizer.from_pretrained('/kaggle/working/tokenizer.pt')\ntest_tokens = tokenizer.batch_encode_plus(\n        eval_dataset.data,\n        padding=True,\n        truncation=True,\n        return_tensors='pt',\n    )\ntest_labels = torch.tensor(eval_dataset.GT)\ntest_data = TensorDataset(test_tokens['input_ids'], test_tokens['attention_mask'], test_labels)\ntest_loader = DataLoader(test_data, batch_size=1, shuffle=False)\nTP,FP,FN = np.zeros((6)),np.zeros((6)),np.zeros((6))\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = batch\n        outputs = model(input_ids.to(device), attention_mask=attention_mask.to(device))\n        outputs = outputs[0].detach().cpu()[0]\n        result = np.zeros((6))\n        probabilities = F.softmax(outputs, dim=0)\n        result[probabilities>0.5] = 1\n        a,b,c = metric(result,labels)\n        TP += a\n        FP += b\n        FN += c\n    print(TP)\n    print(FP)\n    print(FN)\n    F1 = np.zeros(6)\n    for i in range(len(TP)):\n        precision = (TP[i])/(TP[i]+FP[i])\n        recall = (TP[i])/(TP[i]+FN[i])\n        print(precision,recall)\n        F1[i] = (2*precision*recall)/(precision+recall)\n\n    print(sum(F1)/len(F1))","metadata":{"execution":{"iopub.status.busy":"2023-12-04T10:59:00.960029Z","iopub.execute_input":"2023-12-04T10:59:00.960765Z","iopub.status.idle":"2023-12-04T10:59:15.232902Z","shell.execute_reply.started":"2023-12-04T10:59:00.960726Z","shell.execute_reply":"2023-12-04T10:59:15.231953Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"[220.  94. 123. 141. 146. 160.]\n[125.  44.  16.  42.  16.  52.]\n[ 32.  88.  38.  43. 118.  78.]\n0.6376811594202898 0.873015873015873\n0.6811594202898551 0.5164835164835165\n0.8848920863309353 0.7639751552795031\n0.7704918032786885 0.7663043478260869\n0.9012345679012346 0.553030303030303\n0.7547169811320755 0.6722689075630253\n0.7182446527556045\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nclass test_dataset(Dataset):\n    def __init__(self, Data_path):\n        # We should use source id to classify the conservation\n        df = pd.read_csv(Data_path, sep='\\t')\n        self.data = []\n        self.mapping = {'MS':0,\n                       'PH':1,\n                       'AM':2,\n                       'SF':3,\n                       'SR':4,\n                       'OTHER':5}\n        self.GT = []\n        text = []\n        for _, row in df.iterrows():\n            text.append(row['utterance'])\n            train_data = self.get_train_data(text)\n            self.data.append(train_data)\n            GT = np.zeros((6))\n            self.GT.append(GT)\n        \n    def __len__(self):\n        return len(self.data)\n    def get_train_data(self,text):\n        ans = \"\"\n        if(len(text)<3):\n            for i in text:\n                ans += i\n        else:\n            for i in range(3):\n                ans += text[-3+i]\n        return ans \n    def __getitem__(self, idx):\n        data,GT = self.data[idx],self.GT[idx]\n        return data,GT\n\neval_path='/kaggle/input/nlp-hw2/test.tsv'\nfile_id = pd.read_csv('/kaggle/input/nlp-hw2/test.tsv',sep='\\t')\neval_dataset = test_dataset(eval_path)\n# model = BertForSequenceClassification.from_pretrained('/kaggle/working/model.pt').to(device)\n# tokenizer = BertTokenizer.from_pretrained('/kaggle/working/tokenizer.pt')\nmodel = AutoModelForSequenceClassification.from_pretrained('/kaggle/working/model.pt').to(device)\ntokenizer = AutoTokenizer.from_pretrained('/kaggle/working/tokenizer.pt')\ntest_tokens = tokenizer.batch_encode_plus(\n        eval_dataset.data,\n        padding=True,\n        truncation=True,\n        return_tensors='pt',\n    )\ntest_labels = torch.tensor(eval_dataset.GT)\ntest_data = TensorDataset(test_tokens['input_ids'], test_tokens['attention_mask'], test_labels)\ntest_loader = DataLoader(test_data, batch_size=1, shuffle=False)\ndata = {\n    'id': [],\n    'AM': [],\n    'MS': [],\n    'OTHER': [],\n    'PH': [],\n    'SF': [],\n    'SR': [],\n}\n# if('MS' in row['classes']):\n#                 GT[0] = 1\n#             if('PH' in row['classes']):\n#                 GT[1] = 1\n#             if('AM' in row['classes']):\n#                 GT[2] = 1\n#             if('SF' in row['classes']):\n#                 GT[3] = 1\n#             if('SR' in row['classes']):\n#                 GT[4] = 1\n#             if('OTHER' in row['classes']):\n#                 GT[5] = 1\nid = 0\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = batch\n        outputs = model(input_ids.to(device), attention_mask=attention_mask.to(device))\n        outputs = outputs[0].detach().cpu()[0]\n        result = np.zeros((6))\n        probabilities = F.softmax(outputs, dim=0)\n        result[probabilities>0.5] = 1\n        data['id'].append(file_id['id'][id]) \n        if(result[0] == 1):\n            data['MS'].append(1)\n        else:\n            data['MS'].append(0)\n        if(result[1] == 1):\n            data['PH'].append(1)\n        else:\n            data['PH'].append(0)\n        if(result[2] == 1):\n            data['AM'].append(1)\n        else:\n            data['AM'].append(0)\n        if(result[3] == 1):\n            data['SF'].append(1)\n        else:\n            data['SF'].append(0)\n        if(result[4] == 1):\n            data['SR'].append(1)\n        else:\n            data['SR'].append(0)\n        if(result[5] == 1):\n            data['OTHER'].append(1)\n        else:\n            data['OTHER'].append(0)\n        id += 1\nprint(data)\ndf = pd.DataFrame(data)\ndf.to_csv('submit.csv', index=False)\n            \n        ","metadata":{"execution":{"iopub.status.busy":"2023-12-04T10:54:31.605246Z","iopub.execute_input":"2023-12-04T10:54:31.606163Z","iopub.status.idle":"2023-12-04T10:54:39.340443Z","shell.execute_reply.started":"2023-12-04T10:54:31.606107Z","shell.execute_reply":"2023-12-04T10:54:39.339528Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"{'id': [1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847], 'AM': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 'MS': [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'OTHER': [1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1], 'PH': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'SF': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0], 'SR': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}]}]}